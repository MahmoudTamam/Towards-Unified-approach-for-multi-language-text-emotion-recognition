# Unified-Speech-and-Text-Emotion-recognition

### Milestone I :star: Deadline: 8/11
- [x] IEMOCAB data acces on googlecolab.
- [x] Pytorch framework preparation.
- [x] Text model implementation.
- [x] Machine translation survey.
- [x] Spanish | French | German Datasets Text and speech Survey.

### Milestone II: Training Experiment 1 :star: Deadline: 15/11
- [ ] Text preprocessing, Dataloader and Word2vec.
- [ ] Text Agent Implementation, Start training on GoogleColab: IEMOCAB.
- [ ] Text model improvement v1.1.
- [ ] Audio features procoessing, Audio Dataloader implementation.
- [ ] Audio Model branch implementation.

### Milestone III: Training Experiment 2 :star: Deadline: 22/11
- [ ] Text model improvement v1.2.
- [ ] Audio and Text Integration with Attention Experiment.
- [ ] Audio Transcript block.

### Milestone IV: Training Experiment 2 :star: Deadline: 29/11
- [ ] Machine translation and Text/ MultiModel integration vs Survey.
- [ ] MultiModel improvement

### Final Milestone: Training Experiment 2 :star: Deadline: 6/12
- [ ] Paper report.

## Datasets references:
- [EmoDB](http://www.emodb.bilderbar.info/download/) German: Audio and Text --(Ayman) unable to download from server
- [IEMOCAB](https://sail.usc.edu/iemocap/iemocap_release.htm) English: Audio, Visual and Text
- [RAVADES](https://smartlaboratory.org/ravdess/) English: Audio and Visual 25GB.
- [TESS](https://tspace.library.utoronto.ca/handle/1807/24487) English: Audio
- [MSP-Improve](https://ecs.utdallas.edu/research/researchlabs/msp-lab/MSP-Improv.html) English: Audio, Visual
- [MELD](https://affective-meld.github.io/) English: Text, Audio, Visual, 10GB
- [OMGEmotion](https://github.com/knowledgetechnologyuhh/OMGEmotionChallenge) English: TOCHECK
- [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D) English: Audio, Visual, Text
- [RECOLA](https://diuf.unifr.ch/main/diva/recola/index.html) French: Audio, Visual

## Text model emotion recogition references:

## Speech model emotion recogition references:

## Multi model emotion recogition references:
- [Multimodel speech emotion recognition using audio and text](https://arxiv.org/pdf/1810.04635.pdf) (Text and Audio)
- [End to End multimodel emotion recognition using deep learning](https://arxiv.org/pdf/1810.04635.pdf) (Visual and Audio)

## Speech and text machine translation references:

## Word embedding with emphasis on sentiment:
- [Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification](https://www.aclweb.org/anthology/P14-1146/)


### Project milestones

- [ ] Survey: Current Text and speech Emotion Recognition approaches and dataset collection.
- [ ] Pytorch Framework preparition and data preprocessing.
- [ ] Training: Model implementation with training and evalution agents.
- [ ] Text Machine translation block and Audio Machine translation
- [ ] Benchmarking with other langugues state of the art models
- [ ] \(Optional) Blocks merging and optimization
- [ ] \(Optional) Benchmark and compare with Multimodel: Audio and Face emotion recogition approaches
- [ ] We made a contrbution: Conference targeting and opensourcing project ~o~
